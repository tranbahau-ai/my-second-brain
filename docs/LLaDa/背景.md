大規模言語モデル (LLM) は生成モデリングの枠組みに完全に属する。具体的には KL ダイバージェンスの最小化または尤度最大化を通じて真の分布 pdata(⋅)pdata​(⋅) をモデル分布 pθ(⋅)pθ​(⋅) で近似する事を目的とする（式 1）


現在の主流アプローチは自己回帰モデリング (ARM) に基づき以下の形式でモデル分布を定義する


Phương pháp này đã được chứng minh là hiệu quả và trở thành nền tảng cho các LLM hiện đại.
nhưng theo tác giả của bài viết thì autoregressive model – ARM không phải là cách duy nhất để hiện thực hóa khả năng trí tuệ của LLM. Vì:
1. Bản chất cốt lõi của LLM không phải là mô hình tự hồi quy (autoregressive model), mà **nguyên lý mô hình hoá sinh xác suất** (generative modeling principles).
2. Khả năng mở rộng quy mô (scalability) phụ thuộc vào ba yếu tố tương tác: 
	1. **Kiến trúc Transformer**
	2. **Kích thước mô hình và dữ liệu**
	3. **Tính nhất quán Fisher trong nguyên lý hóa sinh (generative principles)**.
3. Các chức năng quan trọng của LLM như **in-context learning** và **instruction following** là những đặc trưng của mô hình điều kiện sinh ngôn ngữ.

Vì thế tác giả đã đề xuất **LLaDA (Large Language Diffusion with mAsking)** – một kiến trúc LLM mới dựa trên **diffusion model** thay vì tự hồi quy.
- Sử dụng **mô hình phân tán với masking (masked diffusion)**.
- Có thể xây dựng mối quan hệ phụ thuộc hai chiều.
- Tối ưu hoá theo một biên dưới xác suất của likelihood.

![[Pasted image 20251217200101.png]]

#### Phương pháp tiếp cận

##### Probabilistic Formulation
Không giống như mô hình tự hồi quy (ARM) sử dụng biểu diễn chuỗi theo thứ tự token, **LLaDA định nghĩa phân phối mô hình $p_{\theta}(x_0)$ thông qua hai quá trình:
