Appendix
=====

Understanding LLM
------------------

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (2018)


Working with Text Data
------------
- [BPE Tokenizer](https://github.com/karpathy/minbpe)

Attention Mechanisms
------------

 - [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) (2017)
 - [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135) (2022)
 - [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691) (2023)

Implementing a GPT model
-------------
 - [Layer Normalization](https://arxiv.org/abs/1607.06450) (2016)
 - [On Layer Normalization in the Transformer Architecture](https://arxiv.org/abs/2002.04745) (2020)
 - [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415) (2016)